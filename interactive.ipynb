{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "First of all, we install all required libraries"
   ],
   "id": "f0bb7a783047dc53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:31:54.738011Z",
     "start_time": "2025-01-12T02:31:52.308206Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -r requirements.txt --quiet",
   "id": "832e559ab5d0a1c1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Inference\n",
    "Here we show how to use the framework for our pretrained models"
   ],
   "id": "2b432ec2c8f0d0f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:07.855625Z",
     "start_time": "2025-01-12T02:33:07.849510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "data_dir = \"data\"\n",
    "dump_dir = os.path.join(data_dir, \"wikimedia_dumps\")\n",
    "extracted_dir = os.path.join(data_dir, \"extracted_txt\")\n",
    "model_dir = \"models\"\n",
    "os.makedirs(dump_dir, exist_ok=True)\n",
    "os.makedirs(extracted_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "n = 3"
   ],
   "id": "dfeedb31568625de",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:08.228192Z",
     "start_time": "2025-01-12T02:33:08.223172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scripts.utils import load_model\n",
    "from scripts.classify import detect_language"
   ],
   "id": "cf212d40b4f1b0b0",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:08.586605Z",
     "start_time": "2025-01-12T02:33:08.581569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_language_models(model_dir):\n",
    "    \"\"\"Load language models from disk.\"\"\"\n",
    "    language_models = {}\n",
    "\n",
    "    for model_file in os.listdir(model_dir):\n",
    "        if model_file.endswith(\"_model.pkl\"):\n",
    "            language = model_file.split(\"_model.pkl\")[0]\n",
    "            model_path = os.path.join(model_dir, model_file)\n",
    "            language_models[language] = load_model(model_path)\n",
    "            print(f\"Model loaded for {language}: {model_path}\")\n",
    "\n",
    "    return language_models"
   ],
   "id": "8e192497704203b7",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:09.116336Z",
     "start_time": "2025-01-12T02:33:09.111478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def classify_text(input_text, language_models, n):\n",
    "    \"\"\"Classify the language of the input text.\"\"\"\n",
    "    probabilities = detect_language(input_text, language_models, n)\n",
    "    return max(probabilities, key=probabilities.get), probabilities"
   ],
   "id": "e2267c56ada77a0c",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:09.700927Z",
     "start_time": "2025-01-12T02:33:09.671212Z"
    }
   },
   "cell_type": "code",
   "source": "loaded_models = load_language_models(model_dir)",
   "id": "ed3ba366d1395f99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded for de: models\\de_model.pkl\n",
      "Model loaded for en: models\\en_model.pkl\n",
      "Model loaded for fr: models\\fr_model.pkl\n",
      "Model loaded for it: models\\it_model.pkl\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here is the inference in action",
   "id": "7818f63ef8406a3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:39.737288Z",
     "start_time": "2025-01-12T02:33:39.732421Z"
    }
   },
   "cell_type": "code",
   "source": "input_text = \"\"\"What? I can tell you something\"\"\"",
   "id": "81acdf0987081e8c",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:40.454264Z",
     "start_time": "2025-01-12T02:33:40.343224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "detected_language, probabilities = classify_text(input_text, loaded_models, n)\n",
    "print(f\"Detected language: {detected_language}\\nProbabilities: {probabilities}\")"
   ],
   "id": "e064b5ca8836f56e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "Probabilities: {'de': np.float64(0.2356590678050734), 'en': np.float64(0.29182772922628053), 'fr': np.float64(0.21953500163136253), 'it': np.float64(0.25297820133728366)}\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:41.109338Z",
     "start_time": "2025-01-12T02:33:41.103908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"\"\"**Die Schönheit der Natur und ihre Bedeutung für den Menschen**\n",
    "\n",
    "Die Natur ist ein unerschöpflicher Quell der Inspiration und des Staunens. Von den majestätischen Bergen, die in den Himmel ragen, bis zu den stillen Wäldern, deren Baumkronen ein grünes Dach bilden, hat die Natur stets eine beruhigende und gleichzeitig belebende Wirkung auf den Menschen. Sie bietet nicht nur einen Rückzugsort aus dem hektischen Alltag, sondern erinnert uns auch an die Einfachheit und den Reichtum des Lebens.\n",
    "\n",
    "In einer Welt, die zunehmend von Technologie und urbanem Lebensstil geprägt ist, scheint der Kontakt zur Natur manchmal verloren zu gehen. Doch gerade dieser Kontakt ist essenziell für das Wohlbefinden von Körper und Geist. Zahlreiche Studien belegen, dass Aufenthalte in der Natur Stress reduzieren, die Kreativität fördern und die allgemeine Lebenszufriedenheit steigern können. Ein einfacher Spaziergang im Park oder eine Wanderung in den Bergen reichen oft aus, um neue Energie zu tanken und den Kopf freizubekommen.\n",
    "\n",
    "Die Natur ist jedoch nicht nur ein Ort der Erholung, sondern auch eine unverzichtbare Lebensgrundlage. Sie liefert uns Nahrung, Wasser und Luft – die Grundelemente des Lebens. Gleichzeitig ist sie ein komplexes Ökosystem, in dem jedes Lebewesen, sei es noch so klein, eine wichtige Rolle spielt. Der Verlust von Artenvielfalt und die Zerstörung von Lebensräumen haben weitreichende Konsequenzen, die nicht nur Tiere und Pflanzen, sondern auch den Menschen betreffen. Daher ist es von großer Bedeutung, die Natur zu schützen und nachhaltige Lebensweisen zu fördern.\n",
    "\n",
    "Ein weiteres faszinierendes Element der Natur ist ihre Fähigkeit zur Regeneration. Auch wenn der Mensch sie oft stark beansprucht und sogar zerstört, zeigt sie immer wieder ihre unglaubliche Widerstandsfähigkeit. Ein Wald, der nach einem Brand wieder ergrünt, oder ein Fluss, der sich nach Jahren der Verschmutzung erholt, sind eindrucksvolle Beispiele dafür. Diese Regenerationsfähigkeit sollte uns jedoch nicht dazu verleiten, sorglos mit der Natur umzugehen. Vielmehr sollte sie uns daran erinnern, dass wir eine Verantwortung tragen, sie für kommende Generationen zu bewahren.\n",
    "\n",
    "Auch kulturell und spirituell spielt die Natur eine zentrale Rolle. Viele Völker und Religionen verehren die Natur als göttlich oder sehen in ihr eine Verbindung zum Höheren. Alte Mythen und Legenden ranken sich um Berge, Flüsse und Wälder, und auch in der modernen Kunst und Literatur ist die Natur ein häufiges Motiv. Diese Verbindung zeigt, wie tief verwurzelt die Beziehung zwischen Mensch und Natur ist.\n",
    "\n",
    "Abschließend bleibt zu sagen, dass die Natur nicht nur ein Ort des Rückzugs oder ein funktionaler Bestandteil unseres Lebens ist, sondern auch eine Quelle der Inspiration, der Erkenntnis und des Friedens. Indem wir uns die Zeit nehmen, sie zu erleben und zu schützen, gewinnen wir nicht nur ein tieferes Verständnis für unsere Umwelt, sondern auch für uns selbst.\"\"\""
   ],
   "id": "ca35222aa29acad8",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "More data gives our model more certainty",
   "id": "f6a42c07a10d4f36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:43.405420Z",
     "start_time": "2025-01-12T02:33:43.292042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "detected_language, p = classify_text(input_text, loaded_models, n)\n",
    "print(f\"Detected language: {detected_language}, probabilities: {p}\")"
   ],
   "id": "2093f3f7c0d2f146",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: de, probabilities: {'de': np.float64(0.9999999056499485), 'en': np.float64(3.395311146388299e-08), 'fr': np.float64(3.3915970895378446e-08), 'it': np.float64(2.648096902140539e-08)}\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Adding new languages and training data",
   "id": "77dc5723b8bfb8dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, we want to download some training data, in order to learn language-representations.\n",
    "\n",
    "The strategy is to build n-gram models of each language, and use that model as a dense representation of that language.\n",
    "We then build an n-gram model of the document to identify, and compare which of the trained models is the closest.\n",
    "\n",
    "Note: The wikimedia data quality unfortunately is a little messed up by the wikimedia foundation, so the training data could be much more improved. Despite this, the model should pick up the language we are looking for."
   ],
   "id": "9cf6fd31de3f9f4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:50.927477Z",
     "start_time": "2025-01-12T02:33:50.921552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file(url, output_path, language_code):\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    with tqdm(total=total_size, unit='B', unit_scale=True, desc=f\"Downloading {language_code}\", leave=True) as pbar:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1 MB chunks\n",
    "                f.write(chunk)\n",
    "                pbar.update(len(chunk))"
   ],
   "id": "bf1cca1f4441455f",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:51.361913Z",
     "start_time": "2025-01-12T02:33:51.357438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def download_wikimedia_dump(language_code, dump_dir):\n",
    "    \"\"\"Download Wikimedia dump for a specific language.\"\"\"\n",
    "    filename = f\"{language_code}wiki-latest-abstract.xml.gz\"\n",
    "    dump_url = f\"https://dumps.wikimedia.org/{language_code}wiki/latest/{filename}\"\n",
    "    dump_file = os.path.join(dump_dir, filename)\n",
    "    if not os.path.exists(dump_file):\n",
    "        download_file(dump_url, dump_file, language_code)\n",
    "    else:\n",
    "        print(f\"Dump already exists for {language_code}.\")\n",
    "    return dump_file"
   ],
   "id": "4eb1932802a12656",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:52.457582Z",
     "start_time": "2025-01-12T02:33:52.447500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_text_from_dump(dump_file, output_dir, language_code, max_docs=5000):\n",
    "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"{language_code}.txt\")\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Output file {output_file} already exists. Skipping processing.\")\n",
    "        return\n",
    "\n",
    "    # Estimate the total number of <doc> tags if no max_docs is specified\n",
    "    total_docs = 0\n",
    "    with gzip.open(dump_file, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if \"<doc>\" in line: total_docs += 1\n",
    "            if total_docs >= max_docs: break\n",
    "\n",
    "    with gzip.open(dump_file, 'rt', encoding='utf-8') as f:\n",
    "        context = ET.iterparse(f, events=('start', 'end'))\n",
    "        _, root = next(context)  # Get the root element\n",
    "\n",
    "        cnt = 0\n",
    "        with tqdm(total=total_docs, desc=\"Extracting articles\", unit=\"doc\", leave=True) as pbar, \\\n",
    "            open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "            for event, elem in context:\n",
    "                if cnt >= max_docs: break\n",
    "                if event == 'end' and elem.tag == 'doc':\n",
    "                    abstract = elem.find('abstract').text\n",
    "\n",
    "                    if abstract: out_file.write(abstract.strip() + \"\\n\")  # Write the abstract to the output file\n",
    "\n",
    "                    pbar.update(1)\n",
    "                    cnt += 1\n",
    "\n",
    "                    # Clear the processed element to save memory\n",
    "                    root.clear()"
   ],
   "id": "aab045f73f860a",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now download and prepare our datasets. Only do this if you want to train a new model! Otherwise the models are already included in the repo",
   "id": "717063628e572039"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:52.956372Z",
     "start_time": "2025-01-12T02:33:52.949831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "language_codes = [\"en\", \"de\", \"fr\", \"it\"] # extend as wished\n",
    "for language_code in language_codes:\n",
    "    dump_file = download_wikimedia_dump(language_code, dump_dir)\n",
    "    extracted_text_dir = extract_text_from_dump(dump_file, extracted_dir, language_code)"
   ],
   "id": "9702da2654e62b79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dump already exists for en.\n",
      "Output file data\\extracted_txt\\en.txt already exists. Skipping processing.\n",
      "Dump already exists for de.\n",
      "Output file data\\extracted_txt\\de.txt already exists. Skipping processing.\n",
      "Dump already exists for fr.\n",
      "Output file data\\extracted_txt\\fr.txt already exists. Skipping processing.\n",
      "Dump already exists for it.\n",
      "Output file data\\extracted_txt\\it.txt already exists. Skipping processing.\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model training",
   "id": "72138f2b40a10031"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we import all the required functionality for model training and comparison. For implementation details, please check the respective files.",
   "id": "35ec5637e0ccd37f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:53.716702Z",
     "start_time": "2025-01-12T02:33:53.711066Z"
    }
   },
   "source": [
    "from scripts.data_loader import load_wikimedia_texts\n",
    "from scripts.preprocessing import preprocess_text\n",
    "from scripts.train import train_language_model\n",
    "from scripts.utils import save_model"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Load and process Wikimedia\n",
    "Our language identifier works by building an n-gram model of the document to classify, and compare it to previously trained n-gram models of various languages.\n",
    "These languages are pulled from wikipedia\n"
   ],
   "id": "be8bc30a8d2f1ccb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:54.339645Z",
     "start_time": "2025-01-12T02:33:54.334148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_preprocess_all_languages(extracted_dir):\n",
    "    language_texts = {}\n",
    "\n",
    "    for file_name in os.listdir(extracted_dir):\n",
    "        if file_name.endswith(\".txt\"):  # Process only .txt files\n",
    "            lang_code = os.path.splitext(file_name)[0]  # Extract language code from filename\n",
    "            file_path = os.path.join(extracted_dir, file_name)\n",
    "\n",
    "            print(f\"Processing language: {lang_code}\")\n",
    "\n",
    "            # Load all lines from the file\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                texts = f.readlines()  # Each line corresponds to one abstract\n",
    "\n",
    "            # Preprocess the texts\n",
    "            preprocessed_texts = [preprocess_text(text.strip()) for text in texts]\n",
    "            language_texts[lang_code] = preprocessed_texts\n",
    "\n",
    "    return language_texts\n"
   ],
   "id": "a8be68d2270d9b65",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:54.651052Z",
     "start_time": "2025-01-12T02:33:54.645712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_language_models(language_models, model_dir):\n",
    "    \"\"\"Save language models to disk.\"\"\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    for language, model in language_models.items():\n",
    "        model_path = os.path.join(model_dir, f\"{language}_model.pkl\")\n",
    "        save_model(model, model_path)\n",
    "        print(f\"Model saved for {language}: {model_path}\")"
   ],
   "id": "3aec602e34a23859",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:54.884252Z",
     "start_time": "2025-01-12T02:33:54.879303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_language_models(language_texts, n):\n",
    "    \"\"\"Train n-gram models for all languages.\"\"\"\n",
    "    language_models = {}\n",
    "\n",
    "    for language, texts in language_texts.items():\n",
    "        print(f\"Training model for {language}\")\n",
    "        language_models[language] = train_language_model(texts, n)\n",
    "\n",
    "    return language_models"
   ],
   "id": "478579ca787d6c71",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:57.906955Z",
     "start_time": "2025-01-12T02:33:55.238120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "language_texts = load_and_preprocess_all_languages(extracted_dir)\n",
    "language_models = train_language_models(language_texts, n)\n",
    "save_language_models(language_models, model_dir)"
   ],
   "id": "9a60781d42f4529b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing language: de\n",
      "Processing language: en\n",
      "Processing language: fr\n",
      "Processing language: it\n",
      "Training model for de\n",
      "Training model for en\n",
      "Training model for fr\n",
      "Training model for it\n",
      "Model saved for de: models\\de_model.pkl\n",
      "Model saved for en: models\\en_model.pkl\n",
      "Model saved for fr: models\\fr_model.pkl\n",
      "Model saved for it: models\\it_model.pkl\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "58765f894a8c6c8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, we can load the new models and run language identification on any text",
   "id": "d869572668ba5d0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:57.945067Z",
     "start_time": "2025-01-12T02:33:57.917560Z"
    }
   },
   "cell_type": "code",
   "source": "loaded_models = load_language_models(model_dir)",
   "id": "fbcc7e3f51bc6051",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded for de: models\\de_model.pkl\n",
      "Model loaded for en: models\\en_model.pkl\n",
      "Model loaded for fr: models\\fr_model.pkl\n",
      "Model loaded for it: models\\it_model.pkl\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:58.036236Z",
     "start_time": "2025-01-12T02:33:58.031273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_classification(input_text):\n",
    "    detected_language, probabilities = classify_text(input_text, loaded_models, n)\n",
    "    print(f\"Detected language: {detected_language}\\nProbabilities: {probabilities}\")"
   ],
   "id": "d9b56796865e0dc5",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T02:33:58.180776Z",
     "start_time": "2025-01-12T02:33:58.069004Z"
    }
   },
   "cell_type": "code",
   "source": "print_classification(\"\"\"La lecture est\"\"\") # Please include at least 3 words",
   "id": "9007048d73453bd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: fr\n",
      "Probabilities: {'de': np.float64(0.23455395834068382), 'en': np.float64(0.24990902768443918), 'fr': np.float64(0.2658008928316076), 'it': np.float64(0.24973612114326935)}\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Congrats, you completed the entire notebook!",
   "id": "cff93dc6b75050fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2c5f38938bdbda67"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
